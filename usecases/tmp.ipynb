{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'just', 'watched', 'a', 'movie', 'called', 'Naruto', 'can', 'you', 'recommend', 'some', 'similar', 'movie']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_entities(question):\n",
    "    # Regular expression pattern to match entities (e.g., \"Naruto\")\n",
    "    pattern = r'\\b\\w+\\b'\n",
    "    \n",
    "    # Find all matches of the pattern in the question\n",
    "    entities = re.findall(pattern, question)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# The last question\n",
    "question = \"I just watched a movie called Naruto, can you recommend some similar movie?\"\n",
    "\n",
    "entities = extract_entities(question)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tongsu/miniconda3/envs/promptir/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import rdflib\n",
    "from rdflib import Graph, URIRef, Literal\n",
    "import re\n",
    "import editdistance\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import llama\n",
    "import csv\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import sentence_BERT\n",
    "\n",
    "\n",
    "# def find_label(s):\n",
    "#     label_space = 'http://www.w3.org/2000/01/rdf-schema#label'\n",
    "#     query_template_label = \"SELECT DISTINCT ?x WHERE {{<{}> <{}> ?x.}}\".format(s, label_space)\n",
    "    \n",
    "#     qres = g.query(query_template_label)\n",
    "#     ass = 'Label not found'\n",
    "#     for row in qres:\n",
    "#         ass = str(row.x)\n",
    "#         break\n",
    "#     return ass\n",
    "\n",
    "g = Graph()\n",
    "g.parse('/home/tongsu/Speakeasy-agent/data/14_graph.nt', format='turtle')\n",
    "\n",
    "node_labels = {}\n",
    "predicate_labels = {}\n",
    "path1 = '/home/tongsu/Speakeasy-agent/data/node_labels.pkl'\n",
    "path2 = '/home/tongsu/Speakeasy-agent/data/predicate_labels.pkl'\n",
    "with open(path1, 'rb') as f:\n",
    "    node_labels = pickle.load(f)\n",
    "with open(path2, 'rb') as f:\n",
    "    predicate_labels = pickle.load(f)\n",
    "\n",
    "\n",
    "def template_1(question):\n",
    "    # # process input\n",
    "    # sent = process_input(question)\n",
    "    # predicted_labels = predict_entities(sent, crf)\n",
    "\n",
    "    # print(\"Input Text:\", question)\n",
    "\n",
    "    # entity = extract_entities(sent, predicted_labels)\n",
    "\n",
    "    # entity = ' '.join(entity)\n",
    "\n",
    "    # #defining a question pattern\n",
    "    # question_pattern = \"who is the (.*) of ENTITY\"\n",
    "\n",
    "    # question = re.sub(entity, \"ENTITY\", question.rstrip(\"?\"))  # preprocess the question\n",
    "\n",
    "    # relation = re.match(question_pattern, question).group(1)  # match the relation using a pattern\n",
    "    \n",
    "    entity, relation = llama.entity_relation_extract(question)\n",
    "    \n",
    "    print(\"!entity:! \", entity)\n",
    "    print(\"!relation:! \", relation)\n",
    "    \n",
    "    relation = sentence_BERT.get_closest(relation)\n",
    "    \n",
    "    print(relation)\n",
    "\n",
    "    #匹配\n",
    "    tmp = 9999\n",
    "    match_node = \"\"\n",
    "    for key, label in node_labels.items():\n",
    "        if editdistance.eval(label, entity) < tmp:\n",
    "            tmp = editdistance.eval(label, entity)\n",
    "            match_node = key\n",
    "\n",
    "    tmp = 9999\n",
    "    match_pred = \"\"\n",
    "    for key, label in predicate_labels.items():\n",
    "        if editdistance.eval(label, relation) < tmp:\n",
    "            tmp = editdistance.eval(label, relation)\n",
    "            match_pred = key\n",
    "\n",
    "    # print(\"\\n--- the matching node of \\\"{}\\\" is {}\\n\".format(entity, match_node))\n",
    "    # print(\"--- the matching predicates of \\\"{}\\\" is {}\\n\".format(relation, match_pred))\n",
    "\n",
    "    #query\n",
    "    query_template = \"SELECT DISTINCT ?x ?y WHERE {{ <{}> <{}> ?x.  }}\".format(match_node, match_pred )\n",
    "\n",
    "    qres = g.query(query_template)\n",
    "    # print(\"\\n--- querying results: \")\n",
    "    answer = 'oops no answer'\n",
    "    for row in qres:\n",
    "        # print(row.x)\n",
    "        print(type(row.x.toPython()))\n",
    "        try:\n",
    "            answer = node_labels[row.x.toPython()]\n",
    "        except KeyError:\n",
    "            answer = row.x\n",
    "    # print(\"answer is:\",format(answer))\n",
    "\n",
    "\n",
    "\n",
    "    WD = rdflib.Namespace('http://www.wikidata.org/entity/')\n",
    "    WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "    DDIS = rdflib.Namespace('http://ddis.ch/atai/')\n",
    "    RDFS = rdflib.namespace.RDFS\n",
    "    SCHEMA = rdflib.Namespace('http://schema.org/')\n",
    "    # load the embeddings\n",
    "    \n",
    "    entity_emb = np.load(\"/home/tongsu/Speakeasy-agent/data/embeddings/entity_embeds.npy\")\n",
    "    relation_emb = np.load(\"/home/tongsu/Speakeasy-agent/data/embeddings/relation_embeds.npy\")\n",
    "    entity_file = \"/home/tongsu/Speakeasy-agent/data/embeddings/entity_ids.del\"\n",
    "    relation_file = \"/home/tongsu/Speakeasy-agent/data/embeddings/relation_ids.del\"\n",
    "    # load the dictionaries\n",
    "    with open(entity_file, 'r') as ifile:\n",
    "        ent2id = {rdflib.term.URIRef(ent): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "        id2ent = {v: k for k, v in ent2id.items()}\n",
    "    with open(relation_file, 'r') as ifile:\n",
    "        rel2id = {rdflib.term.URIRef(rel): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "        id2rel = {v: k for k, v in rel2id.items()}\n",
    "    ent2lbl = {ent: str(lbl) for ent, lbl in g.subject_objects(RDFS.label)}\n",
    "    lbl2ent = {lbl: ent for ent, lbl in ent2lbl.items()}\n",
    "\n",
    "    # Find the Wikidata ID for the movie (https://www.wikidata.org/wiki/Q132863 is the ID for \"Finding Nemo\")\n",
    "    movie = WD[match_node.split('/')[-1]]\n",
    "\n",
    "    # Find the movie in the graph\n",
    "    movie_id = ent2id[movie]\n",
    "\n",
    "    # we compare the embedding of the query entity to all other entity embeddings\n",
    "    distances = pairwise_distances(entity_emb[movie_id].reshape(1, -1), entity_emb, metric='cosine').flatten()\n",
    "\n",
    "    # and sort them by distance\n",
    "    most_likely = distances.argsort()\n",
    "\n",
    "    for rank, idx in enumerate(most_likely[:3]):\n",
    "        rank = rank + 1\n",
    "        ent = id2ent[idx]\n",
    "        q_id = ent.split('/')[-1]\n",
    "        lbl = ent2lbl[ent]\n",
    "        dist = distances[idx]\n",
    "\n",
    "    movie_emb = entity_emb[ent2id[movie]]\n",
    "\n",
    "    try:\n",
    "        # Find the predicate (relation) of the genre (https://www.wikidata.org/wiki/Property:P136 is the ID for \"genre\")\n",
    "        genre = WDT[match_pred.split('/')[-1]]\n",
    "        genre_emb = relation_emb[rel2id[genre]]\n",
    "\n",
    "        # combine according to the TransE scoring function\n",
    "        lhs = movie_emb + genre_emb\n",
    "\n",
    "        # compute distance to *any* entity\n",
    "        distances = pairwise_distances(lhs.reshape(1, -1), entity_emb).reshape(-1)\n",
    "\n",
    "        # find most plausible tails\n",
    "        most_likely = distances.argsort()\n",
    "\n",
    "        embedding_list = []\n",
    "\n",
    "        # show most likely entities\n",
    "        for rank, idx in enumerate(most_likely[:3]):\n",
    "            rank = rank + 1\n",
    "            ent = id2ent[idx]\n",
    "            q_id = ent.split('/')[-1]\n",
    "            lbl = ent2lbl[ent]\n",
    "            dist = distances[idx]\n",
    "            embedding_list.append(lbl)\n",
    "            \n",
    "        answer_embed = ','.join(embedding_list)\n",
    "    \n",
    "    except:\n",
    "        answer_embed = \"oops no answer\"\n",
    "\n",
    "    #answer\n",
    "    if (answer != \"oops no answer\") and (answer_embed != \"oops no answer\"):\n",
    "        answer_template = \"Hi, the factual answer is: {}, \".format(answer) + \"and the answers suggested by embeddings are: {}.\".format(answer_embed)\n",
    "        \n",
    "    elif answer == \"oops no answer\":\n",
    "        answer_template = \"Hi, the answers suggested by embeddings are: {}.\".format(answer_embed)\n",
    "    \n",
    "    elif answer_embed == \"oops no answer\":\n",
    "        answer_template = \"Hi, the factual answer is: {}.\".format(answer)\n",
    "    \n",
    "    else:\n",
    "        answer_template = \"Apologies, but there is no corresponding answer in the database for your question.\"\n",
    "        \n",
    "\n",
    "    # print(\"\\n--- generated response: {}\".format(answer_template))\n",
    "    return answer_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024/10/26 00:53:46 routes.go:1158: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/tongsu/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2024-10-26T00:53:46.253+02:00 level=INFO source=images.go:754 msg=\"total blobs: 6\"\n",
      "time=2024-10-26T00:53:46.255+02:00 level=INFO source=images.go:761 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-10-26T00:53:46.257+02:00 level=INFO source=routes.go:1205 msg=\"Listening on 127.0.0.1:11434 (version 0.3.14)\"\n",
      "time=2024-10-26T00:53:46.260+02:00 level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama3296850918/runners\n"
     ]
    }
   ],
   "source": [
    "command = [\"ollama\", \"serve\"]\n",
    "process = subprocess.Popen(command)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-10-26T00:54:11.979+02:00 level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[rocm_v60102 cpu cpu_avx cpu_avx2 cuda_v11 cuda_v12]\"\n",
      "time=2024-10-26T00:54:11.979+02:00 level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\n",
      "time=2024-10-26T00:54:13.651+02:00 level=INFO source=types.go:123 msg=\"inference compute\" id=GPU-30b4bb22-5580-74ed-7d46-cc6dc249dbb8 library=cuda variant=v12 compute=8.6 driver=12.4 name=\"NVIDIA GeForce RTX 3060 Laptop GPU\" total=\"6.0 GiB\" available=\"5.0 GiB\"\n",
      "time=2024-10-26T00:54:13.900+02:00 level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/home/tongsu/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-30b4bb22-5580-74ed-7d46-cc6dc249dbb8 parallel=4 available=5370806272 required=\"3.7 GiB\"\n",
      "time=2024-10-26T00:54:14.012+02:00 level=INFO source=server.go:105 msg=\"system memory\" total=\"12.3 GiB\" free=\"7.0 GiB\" free_swap=\"3.6 GiB\"\n",
      "time=2024-10-26T00:54:14.013+02:00 level=INFO source=memory.go:326 msg=\"offload to cuda\" layers.requested=-1 layers.model=29 layers.offload=29 layers.split=\"\" memory.available=\"[5.0 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"3.7 GiB\" memory.required.partial=\"3.7 GiB\" memory.required.kv=\"896.0 MiB\" memory.required.allocations=\"[3.7 GiB]\" memory.weights.total=\"2.4 GiB\" memory.weights.repeating=\"2.1 GiB\" memory.weights.nonrepeating=\"308.2 MiB\" memory.graph.full=\"424.0 MiB\" memory.graph.partial=\"570.7 MiB\"\n",
      "time=2024-10-26T00:54:14.017+02:00 level=INFO source=server.go:388 msg=\"starting llama server\" cmd=\"/tmp/ollama3296850918/runners/cuda_v12/ollama_llama_server --model /home/tongsu/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --embedding --n-gpu-layers 29 --threads 8 --parallel 4 --port 46513\"\n",
      "time=2024-10-26T00:54:14.019+02:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
      "time=2024-10-26T00:54:14.019+02:00 level=INFO source=server.go:587 msg=\"waiting for llama runner to start responding\"\n",
      "time=2024-10-26T00:54:14.022+02:00 level=INFO source=server.go:621 msg=\"waiting for server to become available\" status=\"llm server error\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO [main] starting c++ runner | tid=\"140411533201408\" timestamp=1729896854\n",
      "INFO [main] build info | build=10 commit=\"3a8c75e\" tid=\"140411533201408\" timestamp=1729896854\n",
      "INFO [main] system info | n_threads=8 n_threads_batch=8 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"140411533201408\" timestamp=1729896854 total_threads=16\n",
      "INFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"15\" port=\"46513\" tid=\"140411533201408\" timestamp=1729896854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/tongsu/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "time=2024-10-26T00:54:14.526+02:00 level=INFO source=server.go:621 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 24\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 3\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.21 B\n",
      "llm_load_print_meta: model size       = 1.87 GiB (5.01 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   308.23 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  1918.36 MiB\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   896.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.00 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   424.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 902\n",
      "llama_new_context_with_model: graph splits = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO [main] model loaded | tid=\"140411533201408\" timestamp=1729896876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-10-26T00:54:37.135+02:00 level=INFO source=server.go:626 msg=\"llama runner started in 23.12 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/10/26 - 00:54:37 | 200 | 24.054150483s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2024/10/26 - 00:54:37 | 200 |   92.124626ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "!entity:!  Star Wars: Episode VI - Return of the Jedi\n",
      "!relation:!  director\n",
      "director\n",
      "<class 'str'>\n",
      "Hi, the factual answer is: Richard Marquand, and the answers suggested by embeddings are: Frank Oz,George Lucas,Lawrence Kasdan.\n",
      "[GIN] 2024/10/26 - 00:54:52 | 200 |  364.312032ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2024/10/26 - 00:54:52 | 200 |   99.049653ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "!entity:!  The Masked Gang: Cyprus\n",
      "!relation:!  screenwriter\n",
      "screenwriter\n",
      "Hi, the factual answer is: oops no answer, and the answers suggested by embeddings are: Cengiz Küçükayvaz,Murat Aslan,Melih Ekener.\n",
      "[GIN] 2024/10/26 - 00:54:54 | 200 |   202.07919ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2024/10/26 - 00:54:54 | 200 |  139.443954ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "!entity:!  Good Will Hunting\n",
      "!relation:!  director\n",
      "director\n",
      "<class 'str'>\n",
      "Hi, the factual answer is: Gus Van Sant, and the answers suggested by embeddings are: Harmony Korine,Ben Affleck,Gus Van Sant.\n",
      "[GIN] 2024/10/26 - 00:54:57 | 200 |  250.259045ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2024/10/26 - 00:54:57 | 200 |  104.901125ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "!entity:!  The Bridge on the River Kwai\n",
      "!relation:!  director\n",
      "director\n",
      "<class 'str'>\n",
      "Hi, the factual answer is: David Lean, and the answers suggested by embeddings are: Jack Hawkins,David Lean,Harold Goodwin.\n",
      "[GIN] 2024/10/26 - 00:54:58 | 200 |  141.338345ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2024/10/26 - 00:54:58 | 200 |  129.060364ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "!entity:!  Weathering with You\n",
      "!relation:!  MPAA film rating\n",
      "MPA film rating\n",
      "<class 'str'>\n",
      "Hi, the factual answer is: NC-17, and the answers suggested by embeddings are: PG-13,PG,R.\n",
      "[GIN] 2024/10/26 - 00:55:00 | 200 |  115.351613ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2024/10/26 - 00:55:00 | 200 |   92.458696ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "!entity:!  Good Neighbors\n",
      "!relation:!  genre\n",
      "genre\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "Hi, the factual answer is: comedy-drama, and the answers suggested by embeddings are: drama,comedy-drama,comedy film.\n",
      "[GIN] 2024/10/26 - 00:55:02 | 200 |  121.087667ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2024/10/26 - 00:55:02 | 200 |  118.087475ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      "!entity:!  The Godfather\n",
      "!relation:!  release date\n",
      "publication date\n",
      "<class 'datetime.date'>\n",
      "Hi, the factual answer is: 1972-03-15, and the answers suggested by embeddings are: oops no answer.\n"
     ]
    }
   ],
   "source": [
    "Input = [\n",
    "         \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\",\n",
    "         \"Who is the screenwriter of The Masked Gang: Cyprus? \",\n",
    "         \"Who is the director of Good Will Hunting? \",\n",
    "         \"Who directed The Bridge on the River Kwai?\", \n",
    "         \"What is the MPAA film rating of Weathering with You? \",\n",
    "         \"What is the genre of Good Neighbors? \",\n",
    "         'When was \"The Godfather\" released? ',\n",
    "         ]\n",
    "\n",
    "for i in Input:\n",
    "    print(template_1(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
